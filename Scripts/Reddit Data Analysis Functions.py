#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Apr  9 20:54:25 2023

@author: matthewmcmurry
"""

import pandas as pd
import os
import numpy as np
import re
import pandas as pd
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

path = '/Users/matthewmcmurry/Downloads/reddit/subreddits/ZST/CSVs/Segments/Week/'
ending_string = '_Week_1.csv'
#write funciton to search all ending strings for segmenting by week

def ending_strings(path):
    # set the directory path
    dir_path = path
    # create an empty list to store the file name endings
    file_endings = []
    # iterate over the files in the directory
    for filename in os.listdir(dir_path):
    # check if the file name contains 'submission_' or 'comments_'
        if 'submission_' in filename:
            # extract the part of the file name that follows 'submission_'
            file_ending = filename.split('submission_')[1]
            # append the file ending to the list
            file_endings.append(file_ending)
        elif 'comments_' in filename:
            # extract the part of the file name that follows 'comments_'
            file_ending = filename.split('comments_')[1]
            # append the file ending to the list
            file_endings.append(file_ending)
    def file_ending_key(x):
        try:
            return int(x.split('.')[0].split('_')[1])
        except ValueError:
            return x
    file_endings = sorted(set(file_endings), key=file_ending_key)
    return file_endings

def retrieve_dfs(path, file_endings):
    # create an empty dictionary to store the list of dataframes for each file ending
    dfs_dict = {}

    for file_ending in file_endings:
        # filter files that end with the specified string
        csv_files = [f for f in os.listdir(path) if f.endswith(file_ending)]
        dfs = []
        for file in csv_files:
            df = pd.read_csv(os.path.join(path, file))
            dfs.append(df)
        key = file_ending[:-4]
        dfs_dict[key] = dfs
    return dfs_dict
    

def clean_txt(txt_in):
    import regex as re
    clean_str = re.sub("[^A-Za-z0-9]+", " ", txt_in).strip().lower()
    return clean_str

def master_df(my_dict):
    # create an empty list to hold the data for each row in the dataframe
    rows = []
    # loop through each key-value pair in the dictionary
    for segment, dfs_list in my_dict.items():
        # create a new row with the segment name and dfs_list value
        row = {'segment': segment, 'dfs_list': dfs_list}
        # add the row to the list of rows
        rows.append(row)
    # create the dataframe from the list of rows
    master_df = pd.DataFrame(rows)
    return master_df

def merge_dfs(df_list):
    #use df_list in master_df as the input
    merged_df = pd.DataFrame()
    for item in df_list:
        merged_df = pd.concat([merged_df, item], ignore_index=True)
    #create new column with text from 'body' and 'selftext'
    merged_df['text'] = merged_df.apply(lambda row: row['body'] \
        if isinstance(row['body'], str) else row['selftext'], axis=1)
    #remove all rows where the value of text is blank or [removed] or [deleted]
    merged_df = merged_df[(merged_df['text'].apply(lambda x: isinstance(x, str))) & 
        (~merged_df['text'].isin(['[removed]', '[deleted]']))]
    # merged_df['text'] = merged_df['text'].apply(clean_txt)
    return merged_df

def post_counts(dfs_list):
    my_dict = {'comments': 0, 'submissions': 0, 'total posts': 0}
    for df in dfs_list:
        if 'body' in df.columns:
            my_dict['comments'] += len(df)
        else:
            my_dict['submissions'] += len(df)
    my_dict['total posts'] = my_dict['comments'] + my_dict['submissions']
    return my_dict

def sum_dict_values(df, column_name):
    keys = df[column_name].iloc[0].keys()
    sums = dict.fromkeys(keys, 0)

    for row in df[column_name]:
        for key, value in row.items():
            sums[key] += value
    return sums

def generate_wordcloud(my_dict):
    import matplotlib.pyplot as plt
    from wordcloud import WordCloud
    
    wordcloud = WordCloud(width=800, 
                          height=800,
                          background_color='white',
                          max_words=100,
                          random_state=2,
                          max_font_size=100).generate_from_frequencies(my_dict)
    
    plt.figure(figsize=(20, 20))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')

def clean_body(my_col):
    import re
    tmp = my_col
    tmp = tmp.lower()
    #removing auto-generated phrases
    # my_df = my_df.loc[~tmp.str.contains('i am a bot', case=False)]
    #remove specific phrases 
    bot_phrase1 = 'this summary is auto generated by a bot and not meant to replace reading the original article. as always, dyor.'
    bot_phrase2 = 'i am a bot, and this action was performed automatically. please [contact the moderators of this subreddit]'
    del_message = 'delete this message to hide from others.'
    tip_post = 'tip this post'
    q_c = 'if you have any questions or concerns'
    post_removed = 'your post was removed as it mentioned'
    btc_title = 'bitcoin in the title'
    posts_permitted = 'posts that mention an alternative cryptocurrency are only permitted if they also mention ethereum or eth'
    pay2post = 'hi, this comment is being automatically posted under your submission to facilitate the tallying of the pay2post donut penalty that r/ethtrader deducts from user donut earnings for the quantity of posts they submit'
    tmp = tmp.replace(bot_phrase1, '')
    tmp = tmp.replace(bot_phrase2, '')
    tmp = tmp.replace(del_message, '')
    tmp = tmp.replace(pay2post, '')
    tmp = tmp.replace(tip_post, '')
    tmp = tmp.replace(post_removed, '')
    tmp = tmp.replace(posts_permitted, '')
    tmp = tmp.replace(btc_title, '')
    tmp = tmp.replace(q_c, '')
    removal_list = ['feel', 'feels', 'like', 'just', 'really', 'think', 'tipped', 'donut', 'ethtrader',\
                    'ethtrade', 'ethfinance', '&amp;#32;' '&amp;#x200b;', '&amp', 'pretty', '&gt', 'x200b']
    def replace_multiple(var_in, removal_list):
        for string in removal_list:
            var_in = var_in.replace(string, '')
        return var_in
    tmp = replace_multiple(tmp, removal_list)
    #remove hyperlinks
    tmp = re.sub(r'http\S+|www.\S+|.com\S+', '', tmp)
    stop_words = set(stopwords.words('english'))
    remove_stopwords = lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words])
    tmp = remove_stopwords(tmp)
    tmp = re.sub("[^A-Za-z0-9]+", " ", tmp).strip()
    return tmp
    
def get_ngrams(my_col, ngram_min=2, ngram_max=2, max_features=500, n=None):
    import nltk
    import re
    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize
    text = np.array(my_col)
    text = [' '.join(text).lower()]
    vec = CountVectorizer(ngram_range = (ngram_min, ngram_max), 
                          max_features = max_features, 
                          stop_words='english').fit(text)
    bag_of_words = vec.transform(text)
    sum_words = bag_of_words.sum(axis = 0) 
    words_freq = [(word, sum_words[0, i]) for word, i in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)
   
    return words_freq[:n]

def clean_df(my_col):
    for item in my_col:
        tmp = item
        tmp['cleaned'] = tmp['text'].apply(clean_body)
        
def generate_ngrams(my_df):
    my_list = []
    for item in my_df.df:
        # print(item['cleaned'])
        tmp = get_ngrams(item['cleaned'])
        my_list.append(tmp)
    return my_list

def wordclouds_out(my_df):
    for item in my_df.df:
        tmp = ' '.join(np.array(item['cleaned']))


def count_entries(dfs_list):
    combined_df = pd.concat(dfs_list)
    combined_df['created_date'] = pd.to_datetime(combined_df['created_utc'], unit='s').dt.strftime('%Y-%m-%d')
    date_counts = combined_df.groupby('created_date')['created_date'].count().reset_index(name='count')
    return date_counts

def read_csvs_to_df_list(path):
    dfs_list = []
    for file in os.listdir(path):
        if file.endswith(".csv"):
            df = pd.read_csv(os.path.join(path, file))
            dfs_list.append(df)
    return dfs_list









